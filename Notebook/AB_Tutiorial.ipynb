{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3f5825b3-ef40-4c90-9ede-a9b7795fb12a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "  table {\n",
       "    margin-left: 0 !important;\n",
       "  }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "  table {\n",
    "    margin-left: 0 !important;\n",
    "  }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba63390c-6853-43bf-bba9-e12e4e170f6c",
   "metadata": {},
   "source": [
    "# A/B Testing\n",
    "A/B testing is a controlled experiment that compares two versions (A and B) to determine which performs better on a chosen metric, such as conversion rate, click-through rate, or revenue.\n",
    "\n",
    "- Key Points of A/B Testing:\n",
    "    - Random assignment: Users or subjects are randomly split into two groups to test variant A versus variant B.\n",
    "    - Hypotheses:\n",
    "        -  Null Hypothesis ($H_0$): There is no difference between A and B.\n",
    "        -  Alternative Hypothesis ($H_1$): There is a difference (e.g., B performs better than A).\n",
    "    - Data collection: Collect data on user interaction with each variant.\n",
    "    - Statistical analysis: Use hypothesis testing (t-tests, z-tests, or nonparametric tests) to determine if observed differences are statistically significant.\n",
    "    - Decision making: If the null hypothesis is rejected with sufficient evidence, conclude that one variant is superior; otherwise, no difference is confirmed.\n",
    "- Practical Steps:\n",
    "    - Formulate hypotheses.\n",
    "    - Randomly assign users to groups.\n",
    "    - Run the experiment, collecting relevant metrics.\n",
    "    - Perform statistical tests to check for significant differences.\n",
    "    - Implement the winning variant based on results.\n",
    "\n",
    "A/B testing is widely used in product development, marketing, and UX research to optimize user engagement and business outcomes by data-driven decision-making. This method is a real-world application of statistical hypothesis testing designed for practical experiments involving two variants."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2847234-f946-4787-9eae-b7d931226a3d",
   "metadata": {},
   "source": [
    "#### 1. Understand the Experiment Design\n",
    "- Clarify the goal: Increase conversion rate (the number of users who pay for the product).\n",
    "- Identify groups: ‚ÄúControl‚Äù (old page) vs. ‚ÄúTreatment‚Äù (new page).\n",
    "- Know your metrics: Focus on conversion rate, but also review related metrics like average order value or bounce rate for context.\n",
    "\n",
    "#### 2. Load and Explore the Data\n",
    "- Import all relevant data files (typically user-level logs or summaries of visits, conversions, variant assignments, etc.).\n",
    "- Use .head(), .info(), and basic plots (hist(), value_counts()) to get a sense for data shape, missingness, and distributions.\n",
    "\n",
    "#### 3. Check Data Integrity\n",
    "- Ensure randomization: Each user should be assigned to only one group.\n",
    "- Check for duplicates and missing values.\n",
    "- Validate that test groups are balanced in size.\n",
    "\n",
    "#### 4. Define Success Criteria\n",
    "- Decide on the statistical significance threshold ($\\alpha$ is standard).\n",
    "- Set the minimum detectable effect (MDE) your business cares about.\n",
    "- Clarify sample size requirements for adequate statistical power.\n",
    "\n",
    "#### 5. Perform Exploratory Data Analysis (EDA)\n",
    "- Visualize conversion rates in control vs. treatment groups.\n",
    "- Plot histograms or boxplots for order value and other key metrics.\n",
    "- Summarize basic statistics: mean, median, counts, proportions.\n",
    "\n",
    "#### 6. Statistical Testing\n",
    "- Calculate the observed difference in conversion rates.\n",
    "- Use appropriate hypothesis tests (e.g., z-test for proportions, t-test if comparing means) to assess significance.\n",
    "- Consider permutation tests if assumptions for parametric tests are questionable.\n",
    "\n",
    "#### 7. Interpret Results\n",
    "- Compare $\\rho-value$ to the $\\alpha$ threshold.\n",
    "- If significant: consider business and practical impact.\n",
    "- If not significant: review sample size and power‚Äîconsider whether to extend testing.\n",
    "\n",
    "#### 8. Additional Testing and Validation Steps\n",
    "- Common Additional Testing and Validation Steps:\n",
    "    - Permutation (Randomization) Tests: Directly estimate the null distribution of your test statistic by permuting group labels. Confirms robustness if assumptions of traditional parametric tests (like normality) are questionable.\n",
    "    - Bootstrap Confidence Intervals: Resample your observed data to estimate more robust confidence intervals for differences in conversion rate or other metrics.\n",
    "    - Subgroup Analysis: Check if effects are consistent across different customer segments (e.g., geography, device type, user tenure) to rule out confounding or strange heterogeneity.\n",
    "    - Test for Balance: Re-validate that the treatment and control groups are similar in covariates prior to treatment‚Äîimbalance could invalidate causal inference.\n",
    "    - Holdout Validation or Split-Test Replication: Run a smaller version of the experiment (or leave out a random subset as a ‚Äúholdout‚Äù group) to check if effects replicate.\n",
    "    - Power Analysis Post-Hoc: Calculate the observed power of your test to help interpret non-significant results‚Äîis the test underpowered, or is there truly no effect?\n",
    "\n",
    "Note: If your main A/B z-test yields a p-value just above 0.05, running a permutation test and bootstrap interval can confirm whether this result is robust or might vary with sampling noise. Subgroup analysis may reveal that the treatment only helps a specific user group‚Äîaffecting your rollout decision.\n",
    "\n",
    "#### 9. Make and Justify Recommendation\n",
    "- Based on the statistical and practical analysis, advise whether to:\n",
    "    - Roll out the new page,\n",
    "    - Keep the old page,\n",
    "    - Or continue/adjust the experiment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7efec56-d4ef-4720-936a-663f07e5259d",
   "metadata": {},
   "source": [
    "## What is a Hypothesis?\n",
    "In statistics, a hypothesis is a specific, testable statement about a population parameter (like a mean, proportion, or variance).\n",
    "In A/B testing, it‚Äôs used to decide whether an observed difference between two groups (A and B) is real or just due to random chance.\n",
    "\n",
    "#### The Two Competing Hypotheses\n",
    "| Type                  | Symbol      | Meaning                                                              |\n",
    "|-----------------------|-------------|----------------------------------------------------------------------|\n",
    "| Null Hypothesis       | $H_0$     | Assumes no difference between A and B. Any observed difference is due to random variation. |\n",
    "| Alternative Hypothesis | $H_1$ or $H_a$ | Assumes there is a real difference (the new variant changed the metric).                |\n",
    "\n",
    "#### Example: A/B Test on Average Order Value (AOV)\n",
    "You test whether a new marketing strategy (B) increases AOV compared to the current one (A).\n",
    "- $H_0:\\mu_A$ = $\\mu_B$ (no difference in mean AOV)\n",
    "- $H_1:\\mu_B$ > $\\mu_A$ (variant B increases mean AOV)\n",
    "\n",
    "#### One-tailed vs Two-tailed Tests\n",
    "| Test Type   | When to Use                                   | Example                             |\n",
    "|-------------|----------------------------------------------|-----------------------------------|\n",
    "| Two-tailed  | When any difference (increase or decrease) is interesting. | $H_1: \\mu_A \\ne \\mu_B$        |\n",
    "| One-tailed  | When you care about only one direction (e.g., increase).   | $H_1: \\mu_B > \\mu_A$           |\n",
    "\n",
    "**‚ö†Ô∏è One-tailed tests are more powerful but risk bias if the effect goes in the opposite direction.**\n",
    "\n",
    "#### Decision Framework\n",
    "| Step | Concept                       | Description                                                       |\n",
    "| :--- | :---------------------------- | :---------------------------------------------------------------- |\n",
    "| 1    | **State hypotheses**          | Define ($H_0$) and ($H_1$).                                       |\n",
    "| 2    | **Choose significance level** | Typically ($\\alpha$ = 0.05).                                      |\n",
    "| 3    | **Compute test statistic**    | e.g., t, z, U ‚Äî depending on test.                                |\n",
    "| 4    | **Compute p-value**           | Probability of seeing your data if ($H_0$) were true.             |\n",
    "| 5    | **Decision rule**             | If ($\\rho \\le \\alpha$), **reject ($H_0$)** ‚Üí significant difference. |\n",
    "\n",
    "#### Example (t-test)\n",
    "```\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "A = np.random.normal(50, 10, 100)\n",
    "B = np.random.normal(52, 10, 100)\n",
    "\n",
    "tstat, pval = stats.ttest_ind(A, B, equal_var=False)\n",
    "print(f\"t = {tstat:.3f}, p = {pval:.4f}\")\n",
    "```\n",
    "\n",
    "#### Output example:\n",
    "t = -2.041, p = 0.043\n",
    "\n",
    "**Interpretation:** <br>\n",
    "Since p = 0.043 < 0.05, reject $H_0$ <br>\n",
    "The difference between groups is statistically significant. <br>\n",
    "\n",
    "#### Type I and Type II Errors\n",
    "| Error Type        | Symbol | What It Means                                               | Example                                   |\n",
    "| :---------------- | :----- | :---------------------------------------------------------- | :---------------------------------------- |\n",
    "| **Type I Error**  | $\\alpha$      | Rejecting ($H_0$) when it‚Äôs true (false positive).          | You think B works better, but it doesn‚Äôt. |\n",
    "| **Type II Error** | $\\beta$      | Failing to reject ($H_0$) when it‚Äôs false (false negative). | You miss a real improvement.              |\n",
    "\n",
    "**Power = $1 ‚Äì \\beta$ ‚Üí probability of correctly detecting a real effect.**\n",
    "\n",
    "#### Hypothesis Testing in A/B Context\n",
    "| **Metric**                  | **Null Hypothesis ($H_0$)** | **Alternative ($H_1$)** | **Test Used**         |\n",
    "| :-------------------------- | :-------------------------- | :---------------------- | :-------------------- |\n",
    "| Conversion Rate (CR)        | ($p_A = p_B$)               | ($p_A \\ne p_B$)         | Two-proportion z-test |\n",
    "| Average Order Value (AOV)   | ($\\mu_A = \\mu_B$)           | ($\\mu_A \\ne \\mu_B$)     | t-test                |\n",
    "| Orders per User (Frequency) | ($F_A = F_B$)               | ($F_A \\ne F_B$)         | Mann‚ÄìWhitney U test   |\n",
    "| Revenue per Visitor (RPV)   | ($\\mu_A = \\mu_B$)           | ($%\\mu_A \\ne \\mu_B$)    | t-test                |\n",
    "\n",
    "#### Example Summary (A/B test on AOV)\n",
    "| Step                              | Result                                       |\n",
    "| :-------------------------------- | :------------------------------------------- |\n",
    "| ($H_0$): No difference in AOV     |                                              |\n",
    "| ($H_1$): Strategy B increases AOV |                                              |\n",
    "| $\\alpha = 0.05$                   | Significance threshold                       |\n",
    "| $\\rho = 0.082$                    | Computed $\\rho-value$                        |\n",
    "| Decision: Fail to reject ($H_0$)  | No significant difference detected           |\n",
    "| Interpretation                    | Strategy B didn‚Äôt significantly improve AOV. |\n",
    "\n",
    "#### What is AOV (Average Order Value)?\n",
    "AOV stands for Average Order Value, a key e-commerce and marketing performance metric that measures the average amount of money customers spend per order.\n",
    "\n",
    "**Formula:**\n",
    "$$AVO = \\frac {\\text{Total Revenue}}{\\text{Number of Orders}}$$\n",
    "\n",
    "#### Why AOV Matters\n",
    "| **Business Impact**        | **Explanation**                                                                      |\n",
    "| :------------------------- | :----------------------------------------------------------------------------------- |\n",
    "| **Revenue Growth Lever**   | Increasing AOV can grow total revenue without increasing customer acquisition costs. |\n",
    "| **Pricing & Upselling**    | Measures how well your upsells, bundles, and promotions perform.                     |\n",
    "| **Customer Value Insight** | Helps segment high-value vs. low-value customers.                                    |\n",
    "| **Campaign ROI**           | Determines if a new marketing strategy increases purchase size.                      |\n",
    "\n",
    "#### AOV in A/B Testing\n",
    "In A/B testing, AOV is often used as a primary or secondary metric to assess the financial impact of design or pricing changes.\n",
    "|                                              | **Null Hypothesis ($H_0$)** | **Alternative Hypothesis ($H_1$)** |\n",
    "| :------------------------------------------- | :-------------------------- | :--------------------------------- |\n",
    "| **Goal:** Test if new strategy increases AOV | ($\\mu_A = \\mu_B$)           | ($\\mu_B > \\mu_A$)                  |\n",
    "\n",
    "*Where:*\n",
    "- $\\mu_A$: Mean AOV for Control group (A)\n",
    "- $\\mu_B$: Mean AOV for Variant group (B)\n",
    "\n",
    "#### Statistical Test Used\n",
    "| **Condition**                                      | **Test**                      | **Reason**                                           |\n",
    "| :------------------------------------------------- | :---------------------------- | :--------------------------------------------------- |\n",
    "| Large sample (n > 30/group), AOV roughly symmetric | **Two-sample t-test**         | Tests mean difference assuming approximate normality |\n",
    "| Skewed AOV (common in e-commerce) or small samples | **Mann‚ÄìWhitney U test**       | Non-parametric, does not assume normality            |\n",
    "| Very large samples or bootstrapping available      | **Bootstrap mean difference** | Empirical, assumption-free confidence interval       |\n",
    "```\n",
    "# Sample Code\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Example AOVs for A and B\n",
    "A = np.random.normal(50, 15, 1000)\n",
    "B = np.random.normal(52, 15, 1000)\n",
    "\n",
    "# Welch‚Äôs t-test (no equal variance assumption)\n",
    "tstat, pval = stats.ttest_ind(A, B, equal_var=False)\n",
    "print(f\"Mean AOV_A = {A.mean():.2f}, Mean AOV_B = {B.mean():.2f}\")\n",
    "print(f\"t = {tstat:.3f}, p = {pval:.4f}\")\n",
    "```\n",
    "**Output:**\n",
    "Mean AOV_A = 50.12, Mean AOV_B = 52.17 <br>\n",
    "t = -2.331, p = 0.020 <br>\n",
    "\n",
    "**‚úÖ Interpretation:**\n",
    "- $\\rho < 0.05$ ‚Üí reject $H_0$\n",
    "- Strategy B significantly increases AOV.\n",
    "\n",
    "#### Common Pitfalls\n",
    "| **Pitfall**                                       | **Why It‚Äôs a Problem**                          | **Better Approach**                                   |\n",
    "| :------------------------------------------------ | :---------------------------------------------- | :---------------------------------------------------- |\n",
    "| AOV is highly skewed (a few huge orders dominate) | Violates t-test assumptions                     | Use **log-transformed AOV** or **Mann‚ÄìWhitney test**  |\n",
    "| Ignoring conversion rate                          | High AOV but low conversions can reduce revenue | Analyze **Revenue per Visitor (RPV = CR √ó AOV)**      |\n",
    "| Comparing cumulative AOV too early                | Early results fluctuate heavily                 | Use **fixed sample window** or **sequential testing** |\n",
    "| Confusing AOV per user vs per order               | Users may place multiple orders                 | Clarify unit of analysis (order-level vs. user-level) |\n",
    "\n",
    "#### Related Metrics\n",
    "| **Metric**                      | **Formula**                               | **Interpretation**                       |\n",
    "| :------------------------------ | :---------------------------------------- | :--------------------------------------- |\n",
    "| **Conversion Rate (CR)**        | ( \\frac{\\text{Orders}}{\\text{Visitors}} ) | % of visitors who buy                    |\n",
    "| **Revenue per Visitor (RPV)**   | ( \\text{CR} \\times \\text{AOV} )           | Average revenue contribution per visitor |\n",
    "| **Orders per User (Frequency)** | ( \\frac{\\text{Orders}}{\\text{Users}} )    | Customer repeat rate                     |\n",
    "\n",
    "#### ‚úÖ Summary\n",
    "| **Aspect**           | **Details**                                              |\n",
    "| :------------------- | :------------------------------------------------------- |\n",
    "| **Definition**       | Mean order value across all transactions                 |\n",
    "| **Goal in A/B test** | Measure if variant increases transaction size            |\n",
    "| **Typical test**     | Welch‚Äôs t-test or Mann‚ÄìWhitney U test                    |\n",
    "| **Business insight** | AOV increase = users buying more expensive or more items |\n",
    "| **Watch for**        | Skewness, low sample size, ignoring CR impact            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4367cb7-4cd0-43cb-b265-d16eb16f8e47",
   "metadata": {},
   "source": [
    "## What is the Normality Assumption?\n",
    "It‚Äôs the assumption that the data (or more precisely, the sampling distribution of the test statistic) follows a normal (Gaussian) distribution.\n",
    "\n",
    "**In formulas:**\n",
    "- $X \\sim N(\\mu, \\sigma^2)$\n",
    "    - This matters because many classical statistical tests ‚Äî like the t-test ‚Äî are derived under this assumption.\n",
    "\n",
    "#### A/B Testing Context\n",
    "In an A/B test, you compare two versions (A and B) of something (e.g., webpage, email) to see which performs better on a metric (e.g., conversion rate, time spent, click-through rate).\n",
    "\n",
    "Commonly used tests:\n",
    "- Two-sample t-test ‚Äì for comparing means (e.g., average time on site)\n",
    "- Z-test for proportions ‚Äì for comparing rates (e.g., conversion rates)\n",
    "- Nonparametric tests ‚Äì if normality is violated\n",
    "\n",
    "#### When the Normality Assumption Matters\n",
    "| Case                                      | Is Normality Needed? | Why                                                                                                                                                         |\n",
    "| ----------------------------------------- | -------------------- | ---------------------------------------------------------------------------------------------------------------|\n",
    "| **Small sample sizes (n < 30)**           | ‚úÖ Yes                | The t-test assumes data are approximately normal.                                                              |\n",
    "| **Large sample sizes (n ‚â• 30 per group)** | ‚ùå Not strictly       | Thanks to the **Central Limit Theorem (CLT)**, the sampling distribution of the mean (or proportion) becomes approximately normal, even if the data aren‚Äôt. |\n",
    "| **Proportion data (binary outcomes)**     | ‚ùå Not directly       | The Z-test for proportions uses the CLT; normality of the *underlying data* isn‚Äôt required.      |\n",
    "| **Highly skewed data or outliers**        | ‚ö†Ô∏è Maybe not         | Consider data transformation (e.g., log) or a nonparametric alternative.  |\n",
    "\n",
    "#### In Practice\n",
    "- For large-scale A/B tests (typical in web experiments), sample sizes are usually huge ‚Üí normality assumption is not a concern.\n",
    "- For small experiments, check normality:\n",
    "    - Visual check (histogram, Q‚ÄìQ plot)\n",
    "    - Statistical tests (Shapiro‚ÄìWilk, Anderson‚ÄìDarling)\n",
    "- If the assumption fails, use:\n",
    "    - Mann-Whitney U test (for medians instead of means)\n",
    "    - Bootstrap methods (nonparametric confidence intervals)\n",
    "\n",
    "#### ‚úÖ Summary\n",
    "| Question                                 | Answer                                                                                       |\n",
    "| ---------------------------------------- | -------------------------------------------------------------------------------------------- |\n",
    "| Do you need normal data for A/B testing? | Usually, **no**, if you have large samples.                                                   |\n",
    "| Why not?                                 | The **Central Limit Theorem** ensures approximate normality of the sample means/proportions. |\n",
    "| When should you care?                    | When your sample is small or the data are extremely skewed.                                  |\n",
    "\n",
    "#### Normality assumption check\n",
    "- Checking the normality assumption is an important step before applying tests like the t-test in A/B testing (especially with small samples).\n",
    "\n",
    "**Step 1. Clarify What You‚Äôre Testing for Normality**\n",
    "- You‚Äôre testing whether your sample data (e.g., metric values from group A and B) are approximately normally distributed.\n",
    "- In A/B testing:\n",
    "    - If your metric is continuous (e.g., time on site, revenue per user) ‚Üí check normality directly.\n",
    "    - If your metric is binary (e.g., converted / not converted) ‚Üí no need to check; the test statistic (proportion) normality comes from the Central Limit Theorem.\n",
    "      \n",
    "**Step 2. Visual Checks**\n",
    "- Histogram\n",
    "    - Plot a histogram of your metric for each group:\n",
    "        - Should look roughly bell-shaped (symmetrical, unimodal).\n",
    "- Q‚ÄìQ (Quantile‚ÄìQuantile) Plot\n",
    "    - Plots your data‚Äôs quantiles vs. those of a theoretical normal distribution:\n",
    "        - If points fall roughly along a $45 \\degree$ line, ‚Üí data are approximately normal.\n",
    "\n",
    "```\n",
    "# Python example:\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Suppose `group_a` and `group_b` are arrays or Series of your metric\n",
    "sns.histplot(group_a, kde=True)\n",
    "plt.title(\"Histogram of Group A\")\n",
    "plt.show()\n",
    "\n",
    "stats.probplot(group_a, dist=\"norm\", plot=plt)\n",
    "plt.title(\"Q-Q Plot for Group A\")\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Step 3. Statistical Normality Tests**\n",
    "Formal tests for normality (use with caution ‚Äî they‚Äôre sensitive to large sample sizes):\n",
    "\n",
    "- Shapiro‚ÄìWilk Test\n",
    "    - Most common for small to medium samples (n < 5000).\n",
    "\n",
    "```\n",
    "# Sample Python\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "stat, p = shapiro(group_a)\n",
    "print(f\"Statistic={stat:.3f}, p={p:.3f}\")\n",
    "if p > 0.05:\n",
    "    print(\"Sample looks normal (fail to reject H‚ÇÄ).\")\n",
    "else:\n",
    "    print(\"Sample does not look normal (reject H‚ÇÄ).\")\n",
    "\n",
    "```\n",
    "\n",
    "- Kolmogorov‚ÄìSmirnov Test or Anderson‚ÄìDarling Test\n",
    "```\n",
    "from scipy.stats import anderson\n",
    "\n",
    "result = anderson(group_a)\n",
    "print('Statistic: %.3f' % result.statistic)\n",
    "for i in range(len(result.critical_values)):\n",
    "    sl, cv = result.significance_level[i], result.critical_values[i]\n",
    "    if result.statistic < cv:\n",
    "        print(f\"At {sl}% level: data looks normal.\")\n",
    "    else:\n",
    "        print(f\"At {sl}% level: data not normal.\")\n",
    "\n",
    "```\n",
    "**Step 4. Interpret in Context**\n",
    "- If the data look roughly normal, you can use a t-test.\n",
    "- If not normal, but the sample is large (n ‚â• 30), the Central Limit Theorem justifies approximate normality ‚Üí still okay.\n",
    "- If not normal and small sample, use:\n",
    "    - Mann-Whitney U test (nonparametric)\n",
    "    - Or bootstrap confidence intervals\n",
    "\n",
    "**‚úÖ Summary Table**\n",
    "| Method                | Type        | When to Use | Interpretation                       |\n",
    "| --------------------- | ----------- | ----------- | ------------------------------------ |\n",
    "| Histogram             | Visual      | Always      | Rough shape                          |\n",
    "| Q‚ÄìQ Plot              | Visual      | Always      | Deviation from straight line         |\n",
    "| Shapiro‚ÄìWilk          | Statistical | n < 5000    | p > 0.05 ‚Üí normal                    |\n",
    "| Anderson‚ÄìDarling      | Statistical | Any size    | Compare statistic to critical values |\n",
    "| Large sample (n ‚â• 30) | ‚Äî           | ‚Äî           | Normality assumption not critical    |\n",
    "\n",
    "\n",
    "#### Normality Assumption:\n",
    "- The Normality Assumption is one of the core statistical considerations in A/B testing, especially when you‚Äôre using parametric tests like the t-test or z-test.\n",
    "- In an A/B test, you compare the means of two groups (e.g., control vs. variant).\n",
    "If you‚Äôre using a t-test (e.g., scipy.stats.ttest_ind) or z-test, those tests assume that the sampling distribution of the mean is approximately normal.\n",
    "    - The test assumes that the averages (means) you observe across samples follow a normal (bell-shaped) distribution ‚Äî not necessarily that your raw data are perfectly normal.\n",
    "- Parametric tests like the t-test rely on the Central Limit Theorem (CLT), which says:\n",
    "    - When sample sizes are large enough, the distribution of the sample mean tends to be normal, regardless of the shape of the raw data.\n",
    "    - If your sample size is small, non-normality (e.g., skewed data) can bias your test results.\n",
    "    - If your sample size is large, normality of raw data doesn‚Äôt matter much ‚Äî the CLT protects you.  \n",
    "- Case 1: Small Sample, Non-Normal Data\n",
    "    -  Suppose you have only 30 users per group, and the metric is highly skewed (like revenue per user ‚Äî many zeros, a few large spenders).\n",
    "    - The t-test may not be reliable, because the data are not symmetric. In that case, you‚Äôd prefer a non-parametric test (e.g., Mann‚ÄìWhitney U test).\n",
    "- Case 2: Large Sample (Typical A/B Test)\n",
    "    - If you have 1,000+ users per group:\n",
    "        - Even if your revenue or AOV (Average Order Value) data are skewed, the distribution of the sample mean becomes approximately normal.\n",
    "            - ‚úÖ So you can safely use a t-test or z-test. \n",
    "\n",
    "- $H_0$: The assumption of normal distribution is provided\n",
    "- $H_1$: The assumption of normal distribution is not provided\n",
    "\n",
    "If the p-value is less than 0.05, the test is considered significant, and a nonparametric test (Mann-Whitney U test) will be used. Else, a parametric test (t-test)\n",
    "\n",
    "| Condition              | Data Shape         | Sample Size          | Recommended Test      | Why                                   |\n",
    "|------------------------|--------------------|----------------------|----------------------|--------------------------------------|\n",
    "| Roughly normal data    | Symmetric          | Any                  | t-test               | Meets normality assumption directly  |\n",
    "| Skewed data, small n   | Skewed, heavy tails| < 30‚Äì50 per group    | Mann‚ÄìWhitney U test  | Doesn‚Äôt assume normality             |\n",
    "| Skewed data, large n   | Skewed             | ‚â• 100‚Äì200 per group  | t-test (CLT applies) | Sampling distribution ‚âà normal        |\n",
    "| Proportions (e.g., conversion) | Binary outcome    | Large n              | z-test               | Proportion sampling distribution ‚âà normal |\n",
    "\n",
    "```\n",
    "# Sample Code\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example data\n",
    "data_A = [50, 55, 52, 70, 120, 30, 45, 50, 48, 52]\n",
    "\n",
    "# Histogram + Q-Q Plot\n",
    "sns.histplot(data_A, kde=True)\n",
    "stats.probplot(data_A, dist=\"norm\", plot=plt) \n",
    "plt.show()\n",
    "\n",
    "# Shapiro-Wilk normality test\n",
    "stat, p = stats.shapiro(data_A)\n",
    "print(f\"Shapiro-Wilk p-value: {p:.4f}\") \n",
    "if p > 0.05:\n",
    "    print(\"‚úÖ Data looks normal (fail to reject H0).\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Data is likely non-normal (reject H0).\")\n",
    "```\n",
    "#### üßÆ Key Takeaways \n",
    "| ‚úÖ Do‚Äôs                                         | ‚ö†Ô∏è Don‚Äôts                                                         |\n",
    "|------------------------------------------------|------------------------------------------------------------------|\n",
    "| Use t-test if n ‚â• 30 per group (CLT is your friend). | Don‚Äôt assume raw data must be normal ‚Äî it‚Äôs the means that need to be. |\n",
    "| For small or skewed samples, use Mann‚ÄìWhitney U or bootstrap tests. | Don‚Äôt apply t-tests blindly to highly skewed or bounded data (like conversion rates). |\n",
    "| Always visualize distributions (histograms, Q-Q plots). | Don‚Äôt rely only on normality tests for large samples ‚Äî they often flag trivial deviations. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0873a0-ae29-4f84-900b-25251f841778",
   "metadata": {},
   "source": [
    "## Variance Homogeneity\n",
    "One of the key statistical assumptions in A/B testing and t-tests: The assumption of variance homogeneity (also called homoscedasticity).\n",
    "- Variance homogeneity (or equal variance assumption) means\n",
    "    - The spread (variance or standard deviation) of your metric (e.g., AOV, conversion rate, time on site)\n",
    "is roughly the same across all groups being compared.\n",
    "\n",
    "*In A/B testing terms:*\n",
    "- $Var(A) \\approx Var(B)$\n",
    "\n",
    "#### Why It Matters\n",
    "When you run a two-sample t-test, the test formula assumes that both groups have:\n",
    "- Independent samples\n",
    "- Normally distributed means (due to the Central Limit Theorem)\n",
    "- Equal variances (homogeneity)\n",
    "\n",
    "If this assumption is violated:\n",
    "- The standard error of the difference in means is misestimated.\n",
    "- Your $\\rho-values$ and confidence intervals may become inaccurate.\n",
    "\n",
    "**The Two Versions of t-test**\n",
    "| **t-test variant**   | **Assumes equal variances?** | **When to use**                            |\n",
    "| :------------------- | :--------------------------- | :----------------------------------------- |\n",
    "| **Student‚Äôs t-test** | ‚úÖ Yes                        | When variances in both groups are similar  |\n",
    "| **Welch‚Äôs t-test**   | ‚ùå No                         | When variances differ (heteroscedasticity) |\n",
    "\n",
    "**‚úÖ Always safer to use Welch‚Äôs t-test (equal_var=False in Python), since it‚Äôs robust and doesn‚Äôt require equal variances.**\n",
    "\n",
    "```\n",
    "# Example in Python\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Simulate two groups\n",
    "np.random.seed(42)\n",
    "A = np.random.normal(50, 10, 500)   # mean=50, sd=10\n",
    "B = np.random.normal(52, 20, 500)   # mean=52, sd=20 (different variance)\n",
    "\n",
    "# Check variances\n",
    "print(f\"Var(A): {np.var(A, ddof=1):.2f}, Var(B): {np.var(B, ddof=1):.2f}\")\n",
    "\n",
    "# Levene's test for equal variances\n",
    "stat, p = stats.levene(A, B)\n",
    "print(f\"Levene‚Äôs test: W = {stat:.3f}, p = {p:.4f}\")\n",
    "\n",
    "# Choose an appropriate t-test\n",
    "if p < 0.05:\n",
    "    print(\"‚ö†Ô∏è Variances differ ‚Äî use Welch‚Äôs t-test (equal_var=False).\")\n",
    "else:\n",
    "    print(\"‚úÖ Variances are similar ‚Äî standard t-test is fine.\")\n",
    "\n",
    "# Welch‚Äôs t-test (default robust option)\n",
    "t, pval = stats.ttest_ind(A, B, equal_var=False)\n",
    "print(f\"Welch‚Äôs t = {t:.3f}, p = {pval:.4f}\")\n",
    "```\n",
    "**Output Example** <br>\n",
    "Var(A): 95.92, Var(B): 381.21 <br>\n",
    "Levene‚Äôs test: W = 209.307, p = 0.0000 <br>\n",
    "‚ö†Ô∏è Variances differ ‚Äî use Welch‚Äôs t-test (equal_var=False). <br>\n",
    "Welch‚Äôs t = -2.134, p = 0.0331 <br>\n",
    "\n",
    "**‚úÖ Interpretation:**\n",
    "- The variances are significantly different (p < 0.05 from Levene‚Äôs test).\n",
    "- Therefore, use Welch‚Äôs t-test, which adjusts degrees of freedom and handles unequal variances correctly.\n",
    "\n",
    "#### How to Check Variance Homogeneity\n",
    "| **Test / Method**                      | **Purpose**                                    | **Interpretation**              |\n",
    "| :------------------------------------- | :--------------------------------------------- | :------------------------------ |\n",
    "| **Levene‚Äôs Test** (`stats.levene`)     | Most common; robust to non-normality           | ( p > 0.05 ) ‚Üí variances equal  |\n",
    "| **Bartlett‚Äôs Test** (`stats.bartlett`) | Sensitive to normality; use if data are normal | ( p > 0.05 ) ‚Üí variances equal  |\n",
    "| **Visual Inspection**                  | Boxplots or spread plots                       | Compare spread of data visually |\n",
    "\n",
    "```\n",
    "# Example visualization:\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.DataFrame({\"value\": np.concatenate([A, B]),\n",
    "                   \"group\": [\"A\"]*len(A) + [\"B\"]*len(B)})\n",
    "\n",
    "sns.boxplot(data=df, x=\"group\", y=\"value\")\n",
    "plt.title(\"Visual check for variance homogeneity\")\n",
    "plt.show()\n",
    "```\n",
    "#### Summary Table\n",
    "| **Concept**      | **Symbol / Term**                  | **Interpretation**                          |\n",
    "| :--------------- | :--------------------------------- | :------------------------------------------ |\n",
    "| Equal variance   | ($\\sigma^2_A = \\sigma^2_B$)        | Assumed in Student‚Äôs t-test                 |\n",
    "| Unequal variance | ($\\sigma^2_A \\ne \\sigma^2_B$)      | Violates homogeneity                        |\n",
    "| Safe test        | Welch‚Äôs t-test (`equal_var=False`) | Robust to variance differences              |\n",
    "| Check test       | Levene‚Äôs or Bartlett‚Äôs test        | ( p > 0.05 ) ‚Üí OK; ( p < 0.05 ) ‚Üí use Welch |\n",
    "\n",
    "#### Practical Tips in A/B Testing\n",
    "| **Scenario**                                         | **Recommendation**                                                      |\n",
    "| :--------------------------------------------------- | :---------------------------------------------------------------------- |\n",
    "| AOV or revenue metrics (often skewed, high variance) | Use **Welch‚Äôs t-test** by default                                       |\n",
    "| Conversion rate tests                                | Use **proportion z-test** (variance formula known analytically)         |\n",
    "| Small samples with unequal variance                  | Consider **non-parametric test** (Mann‚ÄìWhitney U)                       |\n",
    "| Very large samples                                   | Variance differences have a minor impact, but still report the test type used |\n",
    "\n",
    "**‚úÖ In short:**\n",
    "- Variance homogeneity = equal spread of values between groups.\n",
    "- If violated ‚Üí use Welch‚Äôs t-test.\n",
    "- Always test or visualize before deciding.\n",
    "\n",
    "#### Conceptual Visualization: Variance Homogeneity in t-Tests\n",
    "- We‚Äôd plot two bell curves (the sampling distributions of means for Group A and Group B):\n",
    "1. Equal variance (homoscedastic case):\n",
    "   - Both curves have similar spread (width).\n",
    "   - The t-test assumes this scenario when computing pooled variance.\n",
    "   - The overlap between distributions is symmetrical, so p-values are accurate.\n",
    "2. Unequal variance (heteroscedastic case):\n",
    "   - One curve is much wider (higher variance).\n",
    "   - The assumption of equal spread breaks ‚Äî the standard error is misestimated.\n",
    "   - Student‚Äôs t-test can produce misleading p-values.\n",
    "   - Welch‚Äôs t-test corrects this by using separate variances and adjusted degrees of freedom.\n",
    "\n",
    "| Scenario              | Visualization Idea                         | Interpretation                           |\n",
    "| :-------------------- | :----------------------------------------- | :--------------------------------------- |\n",
    "| **Equal variances**   | Two smooth bell curves with similar widths | ‚úÖ t-test assumption holds                |\n",
    "| **Unequal variances** | One narrow, one wide curve                 | ‚ö†Ô∏è Student‚Äôs t-test invalid; use Welch‚Äôs |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c11d247-80db-46a2-9d7d-5c38474ba2d6",
   "metadata": {},
   "source": [
    "## Start A/B Testing\n",
    "In this A/B test, we are comparing the conversion rates between two groups: the control group (old web page) and the experimental group (new web page). The goal is to determine whether the new web page has a statistically significant effect on the conversion rate, or if any observed difference is due to random chance.\n",
    "\n",
    "Null Hypothesis ($H_0$): The null hypothesis assumes there is no difference in conversion rates between the two groups. In other words, any observed differences are attributed to random variation rather than the design of the new web page.\n",
    "\n",
    "Formally, the null hypothesis:\n",
    "- $H_0: P_{control} = P_{experimental}$\n",
    "- $H_1: P_{control} \\ne P_{experimental}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2166c4f2-1219-4a6f-a163-ba3052f32eaf",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60317785-73cd-4282-8876-82251d846fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read data\n",
    "df = pd.read_csv(\"/Users/sir/Downloads/Chrome/ecommerce_ab_testing_2022_dataset1/ab_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "806b2a9e-d1e1-45b3-9d1f-8445255d03c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>851104</td>\n",
       "      <td>11:48.6</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>804228</td>\n",
       "      <td>01:45.2</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>661590</td>\n",
       "      <td>55:06.2</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>853541</td>\n",
       "      <td>28:03.1</td>\n",
       "      <td>treatment</td>\n",
       "      <td>new_page</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>864975</td>\n",
       "      <td>52:26.2</td>\n",
       "      <td>control</td>\n",
       "      <td>old_page</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id timestamp      group landing_page  converted\n",
       "0   851104   11:48.6    control     old_page          0\n",
       "1   804228   01:45.2    control     old_page          0\n",
       "2   661590   55:06.2  treatment     new_page          0\n",
       "3   853541   28:03.1  treatment     new_page          0\n",
       "4   864975   52:26.2    control     old_page          1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0aaba7b-c79f-433a-bc85-ce5ba334a1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 294480 entries, 0 to 294479\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   user_id       294480 non-null  int64 \n",
      " 1   timestamp     294480 non-null  object\n",
      " 2   group         294480 non-null  object\n",
      " 3   landing_page  294480 non-null  object\n",
      " 4   converted     294480 non-null  int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 11.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86178012-417f-4aaa-acf4-8d2b9a8022f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id         290585\n",
       "timestamp        35993\n",
       "group                2\n",
       "landing_page         2\n",
       "converted            2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45aa313-e51f-4e02-a929-6ec359430eb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00847c29-e2e4-425c-9186-ebac511b72bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da20a589-4016-4a49-befe-30d8b231069b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5610bbba-b107-451b-99b1-787144282747",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7f64e316-bb39-4db2-bc3a-9ca22181d5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required sample size per group: 69768\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from statsmodels.stats.power import zt_ind_solve_power\n",
    "\n",
    "# --- 1. Define Statistical Parameters ---\n",
    "ALPHA = 0.05       # Significance Level (Type I Error)\n",
    "POWER = 0.80       # Statistical Power (1 - Type II Error)\n",
    "\n",
    "# --- 2. Define Business Parameters ---\n",
    "BASELINE_CR_A = 0.150      # Baseline Conversion Rate (e.g., 15.0%)\n",
    "MIN_DETECTABLE_LIFT = 0.10 # Minimum Detectable Relative Lift (e.g., 10%)\n",
    "\n",
    "# Calculate the minimum detectable conversion rate (p_B)\n",
    "# CR_B_MIN = 0.150 √ó (1 + 0.10) = 0.165\n",
    "# The treatment group must reach 16.5% conversion to be considered a meaningful lift.\n",
    "CR_B_MIN = BASELINE_CR_A * (1 + MIN_DETECTABLE_LIFT)\n",
    "\n",
    "# Calculate the effect size (difference between the two proportions)\n",
    "# 0.165 ‚àí 0.150 = 0.015\n",
    "# That‚Äôs a 1.5 percentage point absolute difference.\n",
    "effect_size = CR_B_MIN - BASELINE_CR_A\n",
    "\n",
    "# Then solved for the sample size per group needed to detect that lift with 80% power at 5% significance.\n",
    "n_per_group = zt_ind_solve_power(\n",
    "    effect_size=effect_size,\n",
    "    alpha=ALPHA,\n",
    "    power=POWER,\n",
    "    ratio=1.0,\n",
    "    alternative='two-sided'\n",
    ")\n",
    "print(\"Required sample size per group:\", round(n_per_group))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe959dfc-d566-4314-8569-8f87b758bd61",
   "metadata": {},
   "source": [
    "Interpretation\n",
    "You‚Äôve defined the business lift you care about (10%).\n",
    "\n",
    "Converted it into an absolute difference (1.5 percentage points).\n",
    "\n",
    "Standardized it for the z‚Äëtest.\n",
    "\n",
    "Then solved for the sample size per group needed to detect that lift with 80% power at 5% significance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "94f03e19-6b2f-4c2c-9225-34ef8d8bcbee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized effect size: 0.04200840252084033\n",
      "Required sample size per group: 8895\n"
     ]
    }
   ],
   "source": [
    "p_pool = BASELINE_CR_A  # or use pooled average\n",
    "std_effect_size = effect_size / np.sqrt(p_pool * (1 - p_pool))\n",
    "\n",
    "print(\"Standardized effect size:\", std_effect_size)\n",
    "\n",
    "n_per_group = zt_ind_solve_power(\n",
    "    effect_size=std_effect_size,\n",
    "    alpha=ALPHA,\n",
    "    power=POWER,\n",
    "    ratio=1.0,\n",
    "    alternative='two-sided'\n",
    ")\n",
    "print(\"Required sample size per group:\", round(n_per_group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684b5584-7874-4535-907e-4d8449383efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# # --- 3. Calculate Sample Size per Group ---\n",
    "\n",
    "# # The zt_ind_solve_nobs function requires the normalized effect size (Cohen's h), \n",
    "# # which is automatically calculated based on the two proportions.\n",
    "# # We pass the proportions (prob1 and prob2) to the power function directly.\n",
    "\n",
    "# required_n_per_group = zt_ind_solve_power(\n",
    "#         effect_size=effect_size, \n",
    "#         nobs1=None, \n",
    "#         alpha=ALPHA,\n",
    "#         power=POWER, \n",
    "#         ratio=1.0, \n",
    "#         alternative='two-sided'\n",
    "# )\n",
    "\n",
    "# print(\"--- Sample Size Requirements for Conversion Rate A/B Test ---\")\n",
    "# print(f\"Target Baseline Conversion Rate (pA): {BASELINE_CR_A:.2%}\")\n",
    "# print(f\"Minimum Detectable Relative Lift: {MIN_DETECTABLE_LIFT:.0%}\")\n",
    "# print(f\"Minimum Target Conversion Rate (pB): {CR_B_MIN:.2%}\")\n",
    "# print(\"-\" * 50)\n",
    "# print(f\"Required Sample Size per Group (N): {np.ceil(required_n_per_group):.0f} observations\")\n",
    "# print(f\"Total Required Sample Size (N_A + N_B): {2 * np.ceil(required_n_per_group):.0f} observations\")\n",
    "# print(\"-\" * 50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f361fd3a-d7eb-47be-868a-f5447ab6b8fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42f10b5e-57c1-4955-9a85-12bbb662da3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 294480 entries, 0 to 294479\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count   Dtype \n",
      "---  ------        --------------   ----- \n",
      " 0   user_id       294480 non-null  int64 \n",
      " 1   timestamp     294480 non-null  object\n",
      " 2   group         294480 non-null  object\n",
      " 3   landing_page  294480 non-null  object\n",
      " 4   converted     294480 non-null  int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 11.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6039e377-3ee0-44ea-b87d-162a0c24af93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id         290585\n",
       "timestamp        35993\n",
       "group                2\n",
       "landing_page         2\n",
       "converted            2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce9fd479-b6e5-4e88-a544-7823da933092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id         290585\n",
       "timestamp        35993\n",
       "group                2\n",
       "landing_page         2\n",
       "converted            2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.agg('nunique')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21f81064-5a58-400a-8615-a833bde6cd5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id         0\n",
       "timestamp       0\n",
       "group           0\n",
       "landing_page    0\n",
       "converted       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2bae8a1-a585-4ac8-9879-7234b06d1144",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id         0\n",
       "timestamp       0\n",
       "group           0\n",
       "landing_page    0\n",
       "converted       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f249d0d-7a1e-4b9b-ae15-1377cd783fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Dataset: (294480, 5)\n",
      "New Dataset: (286690, 5)\n"
     ]
    }
   ],
   "source": [
    "# looks like we have duplicate user_id\n",
    "print(\"Original Dataset:\", df.shape)\n",
    "df = df.drop_duplicates(subset=\"user_id\", keep=False)\n",
    "print(\"New Dataset:\", df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf0255a-dd64-4c6a-91c4-481f36301793",
   "metadata": {},
   "source": [
    "### Multiple ways for counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69fc0cde-9ff2-44bb-b138-ebd9c8018ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.4 ms ¬± 364 Œºs per loop (mean ¬± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "# determine counts by group & landing page\n",
    "%timeit df.groupby(['group', 'landing_page']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c27c0d4-fdd0-439e-a4bc-80e37d441971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.3 ms ¬± 491 Œºs per loop (mean ¬± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "# determine counts by group & landing page\n",
    "%timeit df.groupby(['group', 'landing_page']).agg(count=('landing_page', 'count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be64e7e5-1d93-4dd0-b36b-f9fd4244b533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.1 ms ¬± 367 Œºs per loop (mean ¬± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "# determine counts by group & landing page\n",
    "%timeit df.groupby(['group', 'landing_page']).agg(count=('landing_page', 'size'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1d51059-789b-4e7f-b861-7ad9eec46b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.8 ms ¬± 42 Œºs per loop (mean ¬± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "# determine counts by group & landing page using lamda function\n",
    "%timeit df.groupby(['group', 'landing_page']).agg({'landing_page': lambda x: x.value_counts()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e42eb552-8769-406d-917a-d592113b98a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15.9 ms ¬± 274 Œºs per loop (mean ¬± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "# using pivot table\n",
    "%timeit df.groupby(['group', 'landing_page']).size().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e69b31b3-a826-4df5-a43d-36bfe0b92771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>landing_page</th>\n",
       "      <th>new_page</th>\n",
       "      <th>old_page</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>control</th>\n",
       "      <td>0</td>\n",
       "      <td>143293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>treatment</th>\n",
       "      <td>143397</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "landing_page  new_page  old_page\n",
       "group                           \n",
       "control              0    143293\n",
       "treatment       143397         0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determine counts by group & landing page\n",
    "df.groupby(['group', 'landing_page']).size().unstack(fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "032edf7e-b5e6-480d-afbb-c57cb8a472d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "group      landing_page\n",
       "control    old_page        143293\n",
       "treatment  new_page        143397\n",
       "dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['group', 'landing_page']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351b9310-2e26-4087-88f4-34361946e977",
   "metadata": {},
   "source": [
    "### Multiple ways for mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2cad8c85-3b02-4392-9f7b-9134c9075977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>control</th>\n",
       "      <th>old_page</th>\n",
       "      <td>0.120173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>treatment</th>\n",
       "      <th>new_page</th>\n",
       "      <td>0.118726</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        converted\n",
       "group     landing_page           \n",
       "control   old_page       0.120173\n",
       "treatment new_page       0.118726"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the aggregation function mean to the column converted.\n",
    "df.groupby(['group', 'landing_page']).agg({'converted': 'mean'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "827ba37b-ca8d-4701-a923-e811a0758d0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "group      landing_page\n",
       "control    old_page        0.120173\n",
       "treatment  new_page        0.118726\n",
       "Name: converted, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# nly aggregating one column\n",
    "df.groupby(['group', 'landing_page'])['converted'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd7a694c-5051-4d59-9837-f3490cae8b1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>landing_page</th>\n",
       "      <th>new_page</th>\n",
       "      <th>old_page</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>group</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>control</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.120173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>treatment</th>\n",
       "      <td>0.118726</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "landing_page  new_page  old_page\n",
       "group                           \n",
       "control            NaN  0.120173\n",
       "treatment     0.118726       NaN"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pivot this into a table (groups as rows, landing pages as columns)\n",
    "df.pivot_table(index='group', columns='landing_page', values='converted', aggfunc='mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14b3a75-e9ce-405c-a6b3-638a163bf1fe",
   "metadata": {},
   "source": [
    "## Check for Balance\n",
    "- In an A/B test, you want the traffic split between control (old_page) and treatment (new_page) to be roughly equal.\n",
    "- This ensures that any difference in conversion rates is due to the page itself, not because one group had significantly more users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8f65c26a-88a0-4d20-855c-3d2e64d2756d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "landing_page\n",
       "new_page    0.500181\n",
       "old_page    0.499819\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distribution of values\n",
    "df.landing_page.value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4a4e0f19-7239-4686-b766-b492a50b2090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "landing_page\n",
       "new_page    0.500181\n",
       "old_page    0.499819\n",
       "dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby('landing_page').size() / len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "58f5fc21-3994-4658-8e9b-a8392e33a210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>proportion</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>landing_page</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>new_page</th>\n",
       "      <td>0.500181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>old_page</th>\n",
       "      <td>0.499819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              proportion\n",
       "landing_page            \n",
       "new_page        0.500181\n",
       "old_page        0.499819"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pretty dataframe\n",
    "df['landing_page'].value_counts(normalize=True).to_frame('proportion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5245e3ec-d780-499d-8295-78cdf383c594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>landing_page</th>\n",
       "      <th>proportion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>new_page</td>\n",
       "      <td>0.500181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>old_page</td>\n",
       "      <td>0.499819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  landing_page  proportion\n",
       "0     new_page    0.500181\n",
       "1     old_page    0.499819"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pretty dataframe\n",
    "df['landing_page'].value_counts(normalize=True).reset_index(name='proportion')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a318651a-438c-475e-a595-d1e8d65f1570",
   "metadata": {},
   "source": [
    "### Filter out any mismatched rows in the A/B test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5221dab8-1954-497b-a3d0-90eae81c0e05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>group</th>\n",
       "      <th>landing_page</th>\n",
       "      <th>converted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [user_id, timestamp, group, landing_page, converted]\n",
       "Index: []"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.query(\"(group == 'control' and landing_page == 'new_page') or \\\n",
    "          (group == 'treatment' and landing_page == 'old_page')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665a6142-9a38-469c-8d6d-40c58c59fb54",
   "metadata": {},
   "source": [
    "#### Chi-Square Test of Independence\n",
    "- The Chi-Square Test of Independence is a suitable test for your A/B test binary conversion data when you want to evaluate whether the conversion outcome is independent of the group.\n",
    "- Chi-Square Test of Independence provides a powerful, classical method to test if the new page changes conversion rates compared to the old page by assessing the association between conversion and treatment group. assignment (old vs new page).\n",
    "- Hypotheses:\n",
    "    - $H_0$: Conversion rate is independent of group (no difference).\n",
    "    - $H_a$: Conversion rate depends on group (there is a difference)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b09a06d-4404-4768-9e76-d761990d7003",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# counts and totals\n",
    "count_old = df.loc[df.landing_page == 'old_page', 'converted'].sum()\n",
    "n_old =  df.loc[df.landing_page == 'old_page', 'converted'].count()\n",
    "\n",
    "count_new = df.loc[df['landing_page'] == 'new_page', 'converted'].sum()\n",
    "n_new     = df.loc[df['landing_page'] == 'new_page', 'converted'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d54e56-9670-4b8d-b7ae-92bf7e8df8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contingency table: rows = groups, columns = conversion outcomes\n",
    "table = np.array([[count_new, n_new - count_new],\n",
    "                  [count_old, n_old - count_old]])\n",
    "\n",
    "# chi2\n",
    "chi2, p, dof, expected = stats.chi2_contingency(table)\n",
    "\n",
    "print(f\"Chi-square statistic: {chi2:.4f}\")\n",
    "print(f\"P-value: {p:.4f}\")\n",
    "print(f\"Degrees of freedom: {dof}\")\n",
    "print(\"\\nExpected frequencies:\\n\", expected)\n",
    "\n",
    "ci_low, ci_high = sm.stats.confint_proportions_2indep(count_new, n_new, count_old, n_old, method='wald')\n",
    "print(50*'=')\n",
    "print(f\"95% CI for difference in conversion rates: [{ci_low:.4f}, {ci_high:.4f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a46e5b1-06bc-4731-8501-f80cedd51906",
   "metadata": {},
   "source": [
    "#### Proportion Test\n",
    "Null Hypothesis ($H_0$)\n",
    "- $H_0:P_{old} = P_{new}$\n",
    "\n",
    "Alternative Hypothesis ($H_1$)\n",
    "- Two‚Äësided (default):\n",
    "    - $H_1: P_{old} \\ne {P_new}$\n",
    "- ‚Üí The conversion rates are different.\n",
    "    - One‚Äësided (larger): \n",
    "        - $H_1: P_{new} \\gt P_{old}$\n",
    "        - z_stat, p_val = proportion.proportions_ztest(count, nobs, alternative='larger')\n",
    "- ‚Üí The new page has a lower conversion rate.\n",
    "    - One‚Äësided (smaller):\n",
    "        - $H_1: P_{new} < P_{old}$\n",
    "        - z_stat, p_val = proportion.proportions_ztest(count, nobs, alternative='smaller')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be24c9d5-d428-45a2-8eb9-eaf206920131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats import proportion\n",
    "\n",
    "# prpep for test\n",
    "count = [count_old, count_new]\n",
    "nobs  = [n_old, n_new]\n",
    "\n",
    "# Two-sided test (default)\n",
    "z_stat, p_val = proportion.proportions_ztest(count, nobs)\n",
    "print(f\"z-stat: {z_stat:.4f}, p-value: {p_val:.6f}\")\n",
    "\n",
    "# 95% CI for the difference in proportions (new - old), using 'wald' method\n",
    "ci_low, ci_high = proportion.confint_proportions_2indep(count_new, n_new, count_old, n_old, method='wald')\n",
    "print(f\"95% CI (new - old): [{ci_low:.5f}, {ci_high:.5f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e98c64a-6fef-481e-aff7-d7dc84cafbf4",
   "metadata": {},
   "source": [
    "**Interpretation:**\n",
    "- Z-statistic (1.1945): This value quantifies how far the observed difference is from the null hypothesis (no difference), measured in standard errors. A z-score near 0 means little difference; large absolute values indicate larger, more unusual differences if the null hypothesis is true.‚Äã\n",
    "- P-value (0.232288): This is the probability of seeing a difference at least as large as the one observed if there were really no difference between the groups (null hypothesis is true). Because 0.23 is much larger than 0.05:\n",
    "    - Fail to reject the null hypothesis (i.e., not statistically significant) since the difference seen could easily be explained by random chance.\n",
    "- 95% Confidence Interval ([-0.00382, 0.00093]):\n",
    "    - This interval contains 0, meaning the true difference might be negative, positive, or zero.\n",
    "    - There is no statistically significant evidence that the conversion rate for the new page is higher or lower than for the old page\n",
    "\n",
    "Summary: With p-value = 0.23 and a CI spanning zero, you do not have evidence to support rolling out the new page, and cannot reject the ‚Äúno effect‚Äù hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c289e5-6e3d-4312-b08a-5ef630f82c36",
   "metadata": {},
   "source": [
    "The proportion z-test and the chi-square test for two categories are mathematically equivalent in testing differences between two proportions for binary outcomes. The key points on - - when to use each are:\n",
    "    - The proportion z-test is typically used when you want a direct test comparing two proportions and can leverage the normal approximation. It explicitly computes a z-statistic and is conceptually more straightforward when dealing with exactly two categories. It is appropriate when sample sizes are large enough for the normal approximation.\n",
    "    - The chi-square test (with 1 degree of freedom) tests the same hypothesis via a contingency table and evaluates the association between two categorical variables. The chi-square statistic is the square of the z-statistic from the proportion z-test, and either test yields the same p-value.\n",
    "\n",
    "- Historically, the z-test was preferred for two-category cases due to a more straightforward lookup of critical values, but this distinction is now mostly academic given modern computing.\n",
    "    - Use the proportion z-test if you want a straightforward difference in proportions test with normal theory. Use the chi-square test if you prefer the contingency table framework or are dealing with more categories.\n",
    "    - Both tests require sufficiently large samples (expected counts usually ‚â•5) to ensure the chi-square approximation and normal approximation hold.\n",
    "\n",
    "In summary, for the two categories, it doesn't materially matter whether you use a proportion z-test or a chi-square test, as they are effectively the same test with equivalent results. The choice can come down to preference, presentation, or context of analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b121867-d56e-4804-af5f-811d48a90f5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Causal",
   "language": "python",
   "name": "causal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
